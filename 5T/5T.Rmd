---
title: "5T"
author: "Kalyani Cauwenberghs"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

`This is verbatim font.`

###Unsupervised Learning Mini-Project

##1. Exploratory data analysis

```{r}
# Save your input data file to a new 'data' directory
fna.data <- "data/WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("data/WisconsinCancer.csv")
#convert to matrix
wisc.data <- as.matrix(wisc.df)
# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
#create diagnosis vector
diagnosis <-wisc.df$diagnosis
```



We will now exclude id and diagnosis from our data set.

Q1. How many observations are in this dataset?
In the data frame, we have `r nrow(wisc.df)` samples.

Q2. How many of the observations have a malignant diagnosis?
```{r}
a<-table(wisc.df$diagnosis)
a
```
`r as.numeric(a[2])` observations are malignant.

Q3. How many variables/features in the data are suffixed with `_mean`?
```{r}
#returns the column indices
x<-grep("_mean$",colnames(wisc.df))
#returns the column names
x<-grep("_mean$",colnames(wisc.df),value = T)

length(x)
```


##2. Principal Component Analysis
```{r}
#exclude some columns of the data frame, convert to matrix
wisc.df <- subset(wisc.df, select = -c(id, diagnosis,X))
wisc.data <- as.matrix(wisc.df)

# Check column means and standard deviations, round to 3 decimal places
round(colMeans(wisc.data), 3)
round( apply(wisc.data,2,sd), 3 )
```

Because the values are different from one another, we will use `scale=T` when performing PCA.
```{r}
wisc.pr <- prcomp(wisc.data,scale=T)
t<-summary(wisc.pr)
t
#make a scree plot
plot(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
`r t$importance["Proportion of Variance","PC1"]`


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
```{r}
which(t$importance[3,] > 0.7)[1]
```


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
```{r}
which(t$importance[3,] > 0.9)[1]
```


```{r}
biplot(wisc.pr)
```
Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The ont thing that stands out the most is that fractal_dimension_mean is near the bottom. It is difficult to understand because all the words are crowded together.


```{r}
#Plot PC 1 and 2, color by diagnosis
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=diagnosis,xlab="PC1",ylab="PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The red and black dots are a bit more merged in the same space, but still form distinct groups.
```{r}
#Plot PC 1 and 3, color by diagnosis
plot(wisc.pr$x[,1],wisc.pr$x[,3],col=diagnosis,xlab="PC1",ylab="PC3")
```


```{r}
# Calculate variance of each component
eigenvalues <- wisc.pr$sdev^2
wisc.pr.matrix<-rbind(variance = eigenvalues,
  prop_variance = eigenvalues/sum(eigenvalues),
  cum_variance = cumsum(eigenvalues)/sum(eigenvalues))
pr.var <- wisc.pr.matrix["variance",]
head(pr.var)
```






```{r}
# Variance explained by each principal component: pve
pve<-wisc.pr.matrix["prop_variance",]

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```



```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
TODO???


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
TODO see above. The minimum number of principal components required to explain 80% of the variance of the datais  `r wisc.pr.matrix["cum_variance",]`


##3. Hierarchical clustering

```{r}
#scale data
data.scaled <- scale(wisc.data)
#calculate euclidean distances between all pairs of observations
data.dist <- dist(data.scaled,method = "euclidean")
#create a hierarchical clustering model using complete linkage
wisc.hclust <-hclust(data.dist,method="complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
TODO
```{r}
plot(wisc.hclust)
#abline(h=4,col="red", lty=2) TODO fix
```


```{r}
#divide clusters into 4 groups
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)

table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
```{r}
#divide clusters into 4 groups
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)

table(wisc.hclust.clusters, diagnosis)

```




##4. OPTIONAL: K-means clustering

```{r}
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(data.scaled, centers=2, nstart=20)
#create a table
table(wisc.km$cluster,diagnosis)
```

Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
```{r}
table(wisc.km$cluster,wisc.hclust.clusters)
```


##5. Combining methods
```{r}
# Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
#wisc.pr.hclust <- hclust(wisc.pr$x[, 1:7], method="ward.D2")
#cut into 2 clusters
#wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

```

```{r}
#grps <- cutree(wisc.pr.hclust, k=2)
#table(grps)

#table(grps, diagnosis)

#plot(wisc.pr$x[,1:2], col=grps)

#plot(wisc.pr$x[,1:2], col=diagnosis)

#g <- as.factor(grps)
#levels(g)

#g <- relevel(g,2)
#levels(g)

# Plot using our re-ordered factor 
#plot(wisc.pr$x[,1:2], col=g)

#install.packages("rgl")
#library(rgl)
#plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
#for html output
#rglwidget(width = 400, height = 400)
```


Bookmark: https://bioboot.github.io/bimm143_F19/class-material/lab-9-bimm143.html#6_sensitivityspecificity







